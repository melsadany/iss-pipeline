# Pipeline for Iowa Speech Sample (ISS) v.1.10.B

[![Docker](https://img.shields.io/badge/docker-ready-blue)](https://www.docker.com/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

A fully containerized audioâ€‘toâ€‘feature pipeline designed for the **Iowa Speech Sample (ISS)**, version 1.10.B.  
The pipeline processes recorded task audio, performs transcription with **WhisperX**, cleans the transcripts, and extracts a rich set of **linguistic, temporal, semantic, phonetic, and acoustic features** â€“ all orchestrated via Docker.

For a detailed description of the ISS task design, please refer to the provided PDF:  
ðŸ“„ **[ISS_task_design.pdf](config/ISS_task_design.pdf)**

---

## Table of Contents

- [Overview](#overview)
- [Features Extracted](#features-extracted)
- [Requirements](#requirements)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Reference Data](#reference-data)
- [Outputs](#outputs)
- [License & Citation](#license--citation)

---

## Overview

The **ISS (Iowa Speech Sample) Pipeline** automates the analysis of audio recordings from the ISS cognitiveâ€‘linguistic assessment. It:

- Segments audio using a precise task template (CSV timestamps).
- Transcribes each segment with **WhisperX** (wordâ€‘level alignment).
- Cleans transcripts by removing fillers, repetitions, and lowâ€‘confidence words.
- Extracts **perâ€‘word** and **perâ€‘prompt** features:
  - **General linguistic** (AoA, familiarity, concreteness, sentiment)
  - **Temporal** (response latencies, interâ€‘word intervals, burstiness)
  - **Semantic embedding** (sentenceâ€‘transformers, archetypes, spatial anchors)
  - **Phonetic embedding** (PWE, archetypes, norms)
  - **Taskâ€‘specific** (accuracy scores for memory, sentence repetition, reading)

All computations run inside isolated **conda environments** (`whisperx_env`, `pwesuite_env`, `r_pipeline_env`) within a single Docker image â€“ no manual installation needed.

---

## Features Extracted

The pipeline outputs three levels of aggregated data:

| Level            | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **Perâ€‘prompt**   | Features for each prompt.                                                   |
| **Perâ€‘task**     | Averages across prompts, grouped by task type (e.g., letter, lexical).     |
| **Perâ€‘participant** | Singleâ€‘row summary for the entire session.                                 |

### Taskâ€‘Specific Metrics

- **CHECKBOX & HI** â€“ hit rate, response latency.
- **WORD_ASSOC** â€“ constraint adherence, semantic/phonetic diversity, archetype switches, productivity slope, bins statistics, etc.
- **WMEMORY** â€“ recall accuracy (exact match, Levenshtein, phonological similarity, recall order).
- **SENT_REP** â€“ wordâ€‘level and bigram accuracy, word order errors.
- **READING** â€“ correctness per word, reading latency by valence.

### Embeddingâ€‘Based Features

- **Semantic** (384â€‘d from `allâ€‘MiniLMâ€‘L6â€‘v2`):  
  - Similarity to prompt  
  - Pairwise similarities (consecutive / all pairs)  
  - Path length, TSP divergence  
  - Archetype assignment (k=10) and area  
  - Norms (distance from origin / anchors)  
  - Similarity to configâ€‘defined **spatial anchors** (visual, spatial, reasoning)

- **Phonetic** (300â€‘d from PWESuite RNN metric learning model):  
  - Same family of metrics (similarities, archetypes, norms)  
  - Highâ€‘variance dimensions preâ€‘selected


---

## Requirements

- **Docker** (version 20.10+ recommended)
- At least **8 GB RAM** (16 GB preferred for WhisperX largeâ€‘v3)
- **Storage**: ~10 GB for the Docker image + reference data

No local installation of Python, R, or CUDA is required â€“ everything runs inside the container.

---

## Quick Start

1. **Clone the repository** (or download the code and reference data):
   ```bash
   git clone https://github.com/melsadany/iss-pipeline.git
   cd psvc-pipeline
   ```

2. **Place your ISS audio file** (MP3 or WAV) in `test_data/` (create the folder if needed):
   ```bash
   mkdir -p test_data
   cp /path/to/your/recording.mp3 test_data/
   ```

3. **Build the Docker image**:
   ```bash
   docker build -t iss-pipeline:latest .
   ```

4. **Run the pipeline** (replace with participant files and information):
   ```bash
   ./scripts/run_example.sh
   ```

By default, this uses `test_data/test.mp3`. Edit the script to point to your file.

5. **Find the results** in `output/features/.` The main perâ€‘participant file is:
   ```bash
   output/features/TEST0001_per_participant.csv
   ```

---
   
## Configuration

All pipeline parameters are controlled by a single YAML file: `config/task_template.yaml`.
Key sections:

  - task_settings: response windows, valid words for CHECKBOX/HI, constraint rules for WORD_ASSOC.

  - transcription: WhisperX model name (`largeâ€‘v3`), confidence thresholds, filler lists.

  - ground_truth: target sentences for SENT_REP and the reading word list with valence.

  - features: switches to enable/disable acoustic, semantic, phonetic, linguistic, and temporal feature groups.

  - spatial_anchors: userâ€‘defined word lists for similarity calculations (visual, spatial, reasoning).

  - output: what to save (perâ€‘trial, perâ€‘task, perâ€‘participant).

Edit this file to adapt the pipeline to different task versions or to enable/disable specific feature sets.   

---

## Reference Data

The pipeline relies on preâ€‘computed resources located in `reference_data/.` You must provide:

  - `task_metadata/ISS_v_1.10_B_task_metadata.csv` â€“ the ISS template with timestamps for each task prompt.

  - Linguistic norms (AoA, GPT familiarity, MRC, concreteness) â€“ see `00_initialize.R` for expected filenames.

  - Preâ€‘computed embeddings (e`mbeddings/semantic_common_50k.rds`, `phonetic_common_50k.rds`) â€“ or they will be computed onâ€‘theâ€‘fly (slower).

  - Preâ€‘computed archetypes (`archetypes/semantic_common_50k.rds`, `phonetic_common_50k.rds`) â€“ generated by 03_precompute_archetypes.R.

If these files are missing, the pipeline will attempt to create them, but this requires significant time and resources. It is strongly recommended to run `03_precompute_archetypes.R` once on your reference word list (e.g., the 50k most common English words) before processing participants.

---

## Outputs

After a successful run, the `output/` directory contains:

```text
output/
â”œâ”€â”€ cropped_audio/
â”‚   â””â”€â”€ <participant_id>/          # WAV segments
â”œâ”€â”€ transcriptions/
â”‚   â””â”€â”€ <participant_id>/           # Raw WhisperX TSV files
â”œâ”€â”€ review_files/
â”‚   â”œâ”€â”€ <participant_id>_cleaned_transcription.tsv
â”‚   â””â”€â”€ (optionally) ..._REVIEW_REQUIRED.xlsx
â””â”€â”€ features/
    â”œâ”€â”€ <participant_id>_transcription_cleaning_stats.rds
    â”œâ”€â”€ <participant_id>_tasks-minimal-features.rds
    â”œâ”€â”€ <participant_id>_per_prompt.rds
    â”œâ”€â”€ <participant_id>_per_task.rds
    â”œâ”€â”€ <participant_id>_per_participant.rds
    â””â”€â”€ <participant_id>_all_features.rds
```

The most useful for downstream analysis are:

  - `*_per_participant.rds` â€“ one row per participant with all aggregated scores.

  - `*_per_prompt.rds` â€“ detailed data for each fluency prompt, useful for itemâ€‘level analysis.
  
  - `*_per_task.rds` â€“ one row per task type per participant with all aggregated scores.

You can convert these RDS files to CSV using `readRDS()` in R or `pyreadr` in Python.

---

## License & Citation

This pipeline is released under the MIT License.
If you use it in your research, please cite:

    [hj]


Maintainer: Melsadany melsadany24@gmail.com
Repository: https://github.com/melsadany/iss-pipeline